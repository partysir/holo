"""
data_module.py - æ•°æ®ç®¡ç†æ¨¡å— (Tushareç‰ˆ + åŸºæœ¬é¢å› å­)
æ–°å¢žåŠŸèƒ½: ROE, ROA, æ¯›åˆ©çŽ‡, å‡€åˆ©çŽ‡, èµ„äº§è´Ÿå€ºçŽ‡

data_module.py - æ•°æ®ç®¡ç†æ¨¡å— (Tushareç‰ˆ)
è´Ÿè´£: æ•°æ®ç¼“å­˜ã€æ•°æ®èŽ·å–ã€StockRankerå¤šå› å­è®¡ç®—

ä½¿ç”¨å‰å‡†å¤‡:
1. æ³¨å†ŒTushareè´¦å·: https://tushare.pro/register
2. èŽ·å–token: https://tushare.pro/user/token
3. å®‰è£…: pip install tushare pandas numpy
4. è®¾ç½®token: åœ¨main.pyä¸­æ·»åŠ  ts.set_token('ä½ çš„token')
"""

import pandas as pd
import numpy as np
import os
import pickle
import hashlib
import time
from datetime import datetime

# Tushareå¯¼å…¥
try:
    import tushare as ts
    TUSHARE_AVAILABLE = True
except ImportError:
    TUSHARE_AVAILABLE = False
    print("âš ï¸  Tushareæœªå®‰è£…: pip install tushare")


class DataCache:
    """æ•°æ®ç¼“å­˜ç®¡ç†ç±»"""

    def __init__(self, cache_dir='./data_cache'):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        self.cache_dir = cache_dir
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
            print(f"âœ“ åˆ›å»ºç¼“å­˜ç›®å½•: {cache_dir}")

    def _get_cache_key(self, prefix, start_date, end_date, **kwargs):
        """ç”Ÿæˆç¼“å­˜key"""
        key_str = f"{prefix}_{start_date}_{end_date}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def save_to_cache(self, data, cache_name):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        cache_path = os.path.join(self.cache_dir, f"{cache_name}.pkl")
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
            print(f"âœ“ æ•°æ®å·²ç¼“å­˜: {cache_name}")
            return True
        except Exception as e:
            print(f"âœ— ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False

    def load_from_cache(self, cache_name):
        """ä»Žç¼“å­˜åŠ è½½æ•°æ®"""
        cache_path = os.path.join(self.cache_dir, f"{cache_name}.pkl")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    data = pickle.load(f)
                print(f"âœ“ ä»Žç¼“å­˜åŠ è½½: {cache_name}")
                return data
            except Exception as e:
                print(f"âœ— ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        return None

    def save_to_csv(self, df, filename):
        """ä¿å­˜DataFrameåˆ°CSV"""
        csv_path = os.path.join(self.cache_dir, f"{filename}.csv")
        try:
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"âœ“ CSVå·²ä¿å­˜: {filename}")
            return True
        except Exception as e:
            print(f"âœ— CSVä¿å­˜å¤±è´¥: {e}")
            return False

    def load_from_csv(self, filename):
        """ä»ŽCSVåŠ è½½DataFrame"""
        csv_path = os.path.join(self.cache_dir, f"{filename}.csv")
        if os.path.exists(csv_path):
            try:
                df = pd.read_csv(csv_path, encoding='utf-8-sig')
                print(f"âœ“ ä»ŽCSVåŠ è½½: {filename}")
                return df
            except Exception as e:
                print(f"âœ— CSVåŠ è½½å¤±è´¥: {e}")
        return None

    def list_cache_files(self):
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
        if not os.path.exists(self.cache_dir):
            return []
        files = []
        for f in os.listdir(self.cache_dir):
            if f.endswith('.pkl') or f.endswith('.csv'):
                file_path = os.path.join(self.cache_dir, f)
                file_size = os.path.getsize(file_path) / 1024  # KB
                file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                files.append({
                    'name': f,
                    'size_kb': f"{file_size:.2f}",
                    'modified': file_time.strftime('%Y-%m-%d %H:%M:%S')
                })
        return files

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if os.path.exists(self.cache_dir):
            count = 0
            for f in os.listdir(self.cache_dir):
                file_path = os.path.join(self.cache_dir, f)
                try:
                    os.remove(file_path)
                    count += 1
                except Exception as e:
                    print(f"âœ— åˆ é™¤æ–‡ä»¶å¤±è´¥ {f}: {e}")
            print(f"âœ“ å·²æ¸…ç©ºç¼“å­˜ ({count}ä¸ªæ–‡ä»¶)")

class TushareDataSource:
    """Tushareæ•°æ®æºç®¡ç†ç±» - æ‰©å±•åŸºæœ¬é¢æ•°æ®"""

    def __init__(self, cache_manager=None, token=None):
        """åˆå§‹åŒ–Tushareæ•°æ®æº"""
        self.cache = cache_manager

        if not TUSHARE_AVAILABLE:
            raise ImportError("è¯·å…ˆå®‰è£…Tushare: pip install tushare")

        if token:
            ts.set_token(token)

        try:
            self.pro = ts.pro_api()
            print("âœ“ Tushare APIåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âœ— Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
            print("è¯·è®¾ç½®token: ts.set_token('ä½ çš„token')")
            self.pro = None

    def get_stock_list(self, date=None):
        """èŽ·å–è‚¡ç¥¨åˆ—è¡¨"""
        if self.pro is None:
            return []

        try:
            print("ä½¿ç”¨TushareèŽ·å–è‚¡ç¥¨åˆ—è¡¨...")
            df = self.pro.stock_basic(
                exchange='',
                list_status='L',
                fields='ts_code,symbol,name,area,industry,market'
            )

            # è¿‡æ»¤ç§‘åˆ›æ¿(688)ã€åˆ›ä¸šæ¿(300)ã€åŒ—äº¤æ‰€(8/4å¼€å¤´)
            df = df[~df['symbol'].str.startswith(('688', '300', '8', '4'))]

            stock_codes = df['ts_code'].tolist()
            print(f"âœ“ èŽ·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")

            return stock_codes

        except Exception as e:
            print(f"âœ— èŽ·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def get_price_data(self, ts_code, start_date, end_date):
        """èŽ·å–å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®(å¸¦ç¼“å­˜)"""
        if self.pro is None:
            return None

        cache_name = f"price_{ts_code}_{start_date}_{end_date}"
        if self.cache:
            cached_data = self.cache.load_from_cache(cache_name)
            if cached_data is not None:
                return cached_data

        try:
            df = self.pro.daily(
                ts_code=ts_code,
                start_date=start_date.replace('-', ''),
                end_date=end_date.replace('-', ''),
                fields='trade_date,open,high,low,close,vol,amount'
            )

            if df is None or len(df) == 0:
                return None

            df = df.rename(columns={
                'trade_date': 'date',
                'vol': 'volume'
            })

            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
            df['instrument'] = ts_code
            df = df.sort_values('date').reset_index(drop=True)

            result = df[['date', 'instrument', 'open', 'close', 'high', 'low', 'volume']]

            if self.cache:
                self.cache.save_to_cache(result, cache_name)

            time.sleep(0.31)
            return result

        except Exception as e:
            print(f"âœ— èŽ·å– {ts_code} æ•°æ®å¤±è´¥: {e}")
            time.sleep(1)
            return None

    def get_daily_basic(self, ts_code, start_date, end_date):
        """èŽ·å–æ¯æ—¥æŒ‡æ ‡æ•°æ®(PE/PB/PSç­‰)"""
        if self.pro is None:
            return None

        try:
            df = self.pro.daily_basic(
                ts_code=ts_code,
                start_date=start_date.replace('-', ''),
                end_date=end_date.replace('-', ''),
                fields='trade_date,pe,pb,ps,total_mv'
            )

            if df is None or len(df) == 0:
                return None

            df = df.rename(columns={'trade_date': 'date'})
            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
            df['instrument'] = ts_code

            time.sleep(0.31)
            return df

        except Exception as e:
            print(f"âœ— èŽ·å– {ts_code} åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
            return None

    # ========== æ–°å¢ž: è¡Œä¸šæ•°æ®èŽ·å– ==========

    def get_industry_data(self, instruments, use_cache=True):
        """
        èŽ·å–è‚¡ç¥¨è¡Œä¸šæ•°æ®ï¼ˆæ–°å¢žæ–¹æ³• - ä½¿ç”¨ stock_basicï¼‰

        Args:
            instruments: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            DataFrame: [instrument, industry]
        """
        if self.pro is None:
            return pd.DataFrame({
                'instrument': instruments,
                'industry': 'Unknown'
            })

        # æ£€æŸ¥ç¼“å­˜
        cache_name = f"industry_data_all"
        if use_cache and self.cache:
            cached_data = self.cache.load_from_cache(cache_name)
            if cached_data is not None:
                cached_data = cached_data[cached_data['instrument'].isin(instruments)]
                if len(cached_data) > 0:
                    print(f"  âœ“ ä»Žç¼“å­˜åŠ è½½è¡Œä¸šæ•°æ®")
                    return cached_data

        try:
            print(f"  ðŸ“Š èŽ·å– {len(instruments)} åªè‚¡ç¥¨çš„è¡Œä¸šæ•°æ®...")

            # âœ… ä½¿ç”¨ stock_basic èŽ·å–ç”³ä¸‡è¡Œä¸šï¼ˆä¸€æ¬¡è°ƒç”¨èŽ·å–æ‰€æœ‰ï¼‰
            stock_basic = self.pro.stock_basic(
                exchange='',
                list_status='L',
                fields='ts_code,name,industry'
            )

            # ä¿å­˜å®Œæ•´æ•°æ®åˆ°ç¼“å­˜
            if use_cache and self.cache:
                stock_basic_cache = stock_basic.rename(columns={'ts_code': 'instrument'})
                self.cache.save_to_cache(stock_basic_cache[['instrument', 'industry']], cache_name)

            # è¿‡æ»¤ç›®æ ‡è‚¡ç¥¨
            stock_basic = stock_basic[stock_basic['ts_code'].isin(instruments)]
            stock_basic = stock_basic.rename(columns={'ts_code': 'instrument'})
            stock_basic['industry'] = stock_basic['industry'].fillna('å…¶ä»–')

            result = stock_basic[['instrument', 'industry']]

            # è¡¥å……æœªåŒ¹é…çš„è‚¡ç¥¨
            missing = set(instruments) - set(result['instrument'])
            if missing:
                print(f"  âš ï¸  {len(missing)} åªè‚¡ç¥¨æœªæ‰¾åˆ°è¡Œä¸šï¼Œæ ‡è®°ä¸º'å…¶ä»–'")
                missing_df = pd.DataFrame({
                    'instrument': list(missing),
                    'industry': 'å…¶ä»–'
                })
                result = pd.concat([result, missing_df], ignore_index=True)

            print(f"  âœ“ è¡Œä¸šæ•°æ®èŽ·å–å®Œæˆ")
            print(f"     è¦†ç›–çŽ‡: {(len(result) - len(missing)) / len(instruments) * 100:.1f}%")
            print(f"     è¡Œä¸šæ•°: {result['industry'].nunique()}ä¸ª")

            # æ˜¾ç¤ºè¡Œä¸šåˆ†å¸ƒ
            top_industries = result['industry'].value_counts().head(5)
            print(f"     TOP5è¡Œä¸š:")
            for ind, cnt in top_industries.items():
                print(f"       - {ind}: {cnt}åª")

            return result

        except Exception as e:
            print(f"  âš ï¸  èŽ·å–è¡Œä¸šæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame({
                'instrument': instruments,
                'industry': 'Unknown'
            })
    # ========== æ–°å¢ž: åŸºæœ¬é¢è´¢åŠ¡æ•°æ®èŽ·å– ==========

    def get_financial_indicators(self, ts_code, start_date, end_date):
        """
        èŽ·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ®(ROE, ROA, æ¯›åˆ©çŽ‡, å‡€åˆ©çŽ‡, èµ„äº§è´Ÿå€ºçŽ‡)
        :param ts_code: è‚¡ç¥¨ä»£ç 
        :param start_date: å¼€å§‹æ—¥æœŸ
        :param end_date: ç»“æŸæ—¥æœŸ
        :return: DataFrame
        """
        if self.pro is None:
            return None

        # æ£€æŸ¥ç¼“å­˜
        cache_name = f"financial_{ts_code}_{start_date}_{end_date}"
        if self.cache:
            cached_data = self.cache.load_from_cache(cache_name)
            if cached_data is not None:
                return cached_data

        try:
            # èŽ·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ®
            df = self.pro.fina_indicator(
                ts_code=ts_code,
                start_date=start_date.replace('-', ''),
                end_date=end_date.replace('-', ''),
                fields='ts_code,ann_date,end_date,roe,roa,grossprofit_margin,netprofit_margin,debt_to_assets'
            )

            if df is None or len(df) == 0:
                return None

            # æ•°æ®é¢„å¤„ç†
            df = df.rename(columns={
                'ann_date': 'date',  # ä½¿ç”¨å…¬å‘Šæ—¥æœŸ
                'grossprofit_margin': 'gross_margin',
                'netprofit_margin': 'net_margin',
                'debt_to_assets': 'debt_ratio'
            })

            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
            df['instrument'] = ts_code

            # æŒ‰æ—¥æœŸæŽ’åº
            df = df.sort_values('date').reset_index(drop=True)

            # ä¿å­˜åˆ°ç¼“å­˜
            if self.cache:
                self.cache.save_to_cache(df, cache_name)

            time.sleep(0.31)  # APIé™æµ
            return df

        except Exception as e:
            print(f"âœ— èŽ·å– {ts_code} è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {e}")
            time.sleep(1)
            return None

    def merge_financial_data_to_daily(self, price_df, financial_df):
        """
        å°†å­£åº¦è´¢åŠ¡æ•°æ®åˆå¹¶åˆ°æ—¥çº¿æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰
        ä½¿ç”¨å‰å‘å¡«å……æ–¹æ³•:æ¯ä¸ªäº¤æ˜“æ—¥ä½¿ç”¨æœ€è¿‘å…¬å‘Šçš„è´¢åŠ¡æ•°æ®

        :param price_df: æ—¥çº¿ä»·æ ¼æ•°æ®
        :param financial_df: è´¢åŠ¡æŒ‡æ ‡æ•°æ®
        :return: åˆå¹¶åŽçš„DataFrame
        """
        if financial_df is None or len(financial_df) == 0:
            print("  âš ï¸  è´¢åŠ¡æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆå¹¶")
            return price_df

        # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´
        price_df['date'] = pd.to_datetime(price_df['date'], errors='coerce')
        financial_df['date'] = pd.to_datetime(financial_df['date'], errors='coerce')

        financial_df = financial_df.dropna(subset=['date', 'instrument'])
        price_df = price_df.dropna(subset=['date', 'instrument'])

        if len(financial_df) == 0:
            return price_df
        # ========== å…³é”®ä¿®å¤ï¼šæ¸…ç†ç©ºå€¼ ==========
        print("  ðŸ” æ¸…ç†æ•°æ®ç©ºå€¼...")

        # 1. æ¸…ç†è´¢åŠ¡æ•°æ®ä¸­çš„ç©ºå€¼
        original_len = len(financial_df)
        financial_df = financial_df.dropna(subset=['date', 'instrument'])
        cleaned_len = len(financial_df)

        if original_len > cleaned_len:
            print(f"     è´¢åŠ¡æ•°æ®: ç§»é™¤ {original_len - cleaned_len} æ¡ç©ºå€¼è®°å½•")

        # 2. æ¸…ç†ä»·æ ¼æ•°æ®ä¸­çš„ç©ºå€¼
        price_df = price_df.dropna(subset=['date', 'instrument'])

        if len(financial_df) == 0:
            print("  âš ï¸  è´¢åŠ¡æ•°æ®æ¸…ç†åŽä¸ºç©ºï¼Œè·³è¿‡åˆå¹¶")
            return price_df

        # å¯¹æ¯åªè‚¡ç¥¨å•ç‹¬å¤„ç†
        result_list = []
        success_count = 0
        fail_count = 0

        print("  ðŸ”— åˆå¹¶è´¢åŠ¡æ•°æ®åˆ°æ—¥çº¿...")
        instruments = price_df['instrument'].unique()

        for idx, instrument in enumerate(instruments):
            if (idx + 1) % 500 == 0:
                print(f"     è¿›åº¦: {idx + 1}/{len(instruments)}")

            price_subset = price_df[price_df['instrument'] == instrument].copy()
            financial_subset = financial_df[financial_df['instrument'] == instrument].copy()

            if len(financial_subset) == 0:
                # æ²¡æœ‰è´¢åŠ¡æ•°æ®ï¼Œç›´æŽ¥ä½¿ç”¨ä»·æ ¼æ•°æ®
                result_list.append(price_subset)
                continue

            # å†æ¬¡ç¡®ä¿å½“å‰è‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®æ²¡æœ‰ç©ºå€¼
            financial_subset = financial_subset.dropna(subset=['date'])

            if len(financial_subset) == 0:
                result_list.append(price_subset)
                continue

            try:
                # ä½¿ç”¨merge_asofè¿›è¡Œå‰å‘å¡«å……åˆå¹¶
                merged = pd.merge_asof(
                    price_subset.sort_values('date'),
                    financial_subset.sort_values('date')[
                        ['date', 'roe', 'roa', 'gross_margin', 'net_margin', 'debt_ratio']
                    ],
                    on='date',
                    direction='backward'  # å‘åŽæŸ¥æ‰¾æœ€è¿‘çš„è´¢åŠ¡æ•°æ®
                )
                result_list.append(merged)
                success_count += 1

            except Exception as e:
                # åˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨åŽŸå§‹ä»·æ ¼æ•°æ®
                print(f"     âš ï¸  {instrument} åˆå¹¶å¤±è´¥: {e}")
                result_list.append(price_subset)
                fail_count += 1

        if len(result_list) == 0:
            print("  âš ï¸  æ²¡æœ‰å¯åˆå¹¶çš„æ•°æ®")
            return price_df

        result_df = pd.concat(result_list, ignore_index=True)

        # ç»Ÿè®¡ä¿¡æ¯
        print(f"  âœ“ åˆå¹¶å®Œæˆ:")
        print(f"     æˆåŠŸ: {success_count} åª")
        if fail_count > 0:
            print(f"     å¤±è´¥: {fail_count} åª")

        # ç»Ÿè®¡åŸºæœ¬é¢æ•°æ®è¦†ç›–çŽ‡
        fundamental_cols = ['roe', 'roa', 'gross_margin', 'net_margin', 'debt_ratio']
        available_cols = [col for col in fundamental_cols if col in result_df.columns]

        if available_cols:
            has_data = result_df[available_cols].notna().any(axis=1)
            coverage = (has_data.sum() / len(result_df)) * 100
            print(f"     è¦†ç›–çŽ‡: {coverage:.1f}%")

        return result_df

class StockRankerModel:
    """
    StockRanker å¤šå› å­è¯„åˆ†æ¨¡åž‹ (æ‰©å±•ç‰ˆ)
    æ•´åˆ: ä¼°å€¼ã€æ³¢åŠ¨çŽ‡ã€èµ„é‡‘æµã€åŠ¨é‡ã€åŸºæœ¬é¢å› å­
    """

    def __init__(self, custom_weights=None, use_fundamental=True):
        """
        åˆå§‹åŒ–æ¨¡åž‹
        :param custom_weights: è‡ªå®šä¹‰å› å­æƒé‡å­—å…¸
        :param use_fundamental: æ˜¯å¦ä½¿ç”¨åŸºæœ¬é¢å› å­
        """
        self.use_fundamental = use_fundamental

        if custom_weights:
            self.factor_weights = custom_weights
        else:
            # é»˜è®¤æƒé‡é…ç½® (åŒ…å«åŸºæœ¬é¢å› å­)
            if use_fundamental:
                self.factor_weights = {
                    # ä¼°å€¼å› å­ (25%) - è¶Šä½Žè¶Šå¥½
                    'pe_ratio': -0.10,
                    'pb_ratio': -0.10,
                    'ps_ratio': -0.05,

                    # æ³¢åŠ¨çŽ‡å› å­ (15%) - è¶Šä½Žè¶Šå¥½
                    'volatility_20d': -0.08,
                    'volatility_60d': -0.07,

                    # èµ„é‡‘æµå› å­ (15%) - è¶Šé«˜è¶Šå¥½
                    'money_flow_20d': 0.08,
                    'volume_ratio': 0.07,

                    # åŠ¨é‡å› å­ (15%) - è¶Šé«˜è¶Šå¥½
                    'return_20d': 0.08,
                    'return_60d': 0.07,

                    # åŸºæœ¬é¢å› å­ (30%) - æ–°å¢ž
                    'roe': 0.10,  # ROEè¶Šé«˜è¶Šå¥½
                    'roa': 0.05,  # ROAè¶Šé«˜è¶Šå¥½
                    'gross_margin': 0.05,  # æ¯›åˆ©çŽ‡è¶Šé«˜è¶Šå¥½
                    'net_margin': 0.05,  # å‡€åˆ©çŽ‡è¶Šé«˜è¶Šå¥½
                    'debt_ratio': -0.05  # èµ„äº§è´Ÿå€ºçŽ‡è¶Šä½Žè¶Šå¥½
                }
            else:
                # ä¸ä½¿ç”¨åŸºæœ¬é¢å› å­çš„åŽŸå§‹é…ç½®
                self.factor_weights = {
                    'pe_ratio': -0.15,
                    'pb_ratio': -0.15,
                    'ps_ratio': -0.10,
                    'volatility_20d': -0.10,
                    'volatility_60d': -0.10,
                    'money_flow_20d': 0.10,
                    'volume_ratio': 0.10,
                    'return_20d': 0.10,
                    'return_60d': 0.10
                }

        print("\n" + "=" * 60)
        print("ðŸ“Š StockRanker å¤šå› å­è¯„åˆ†æ¨¡åž‹")
        if use_fundamental:
            print("    âœ¨ åŸºæœ¬é¢å› å­å·²å¯ç”¨")
        print("=" * 60)
        self._print_weights()

    def _print_weights(self):
        """æ‰“å°å› å­æƒé‡é…ç½®"""
        print("\nå› å­æƒé‡é…ç½®:")

        # ä¼°å€¼å› å­
        print("  â”œâ”€ ä¼°å€¼å› å­ (25%)" if self.use_fundamental else "  â”œâ”€ ä¼°å€¼å› å­ (40%)")
        print(f"  â”‚   â”œâ”€ PEå¸‚ç›ˆçŽ‡: {self.factor_weights.get('pe_ratio', 0):.2%}")
        print(f"  â”‚   â”œâ”€ PBå¸‚å‡€çŽ‡: {self.factor_weights.get('pb_ratio', 0):.2%}")
        print(f"  â”‚   â””â”€ PSå¸‚é”€çŽ‡: {self.factor_weights.get('ps_ratio', 0):.2%}")

        # æ³¢åŠ¨çŽ‡å› å­
        print("  â”œâ”€ æ³¢åŠ¨çŽ‡å› å­ (15%)" if self.use_fundamental else "  â”œâ”€ æ³¢åŠ¨çŽ‡å› å­ (20%)")
        print(f"  â”‚   â”œâ”€ 20æ—¥æ³¢åŠ¨çŽ‡: {self.factor_weights.get('volatility_20d', 0):.2%}")
        print(f"  â”‚   â””â”€ 60æ—¥æ³¢åŠ¨çŽ‡: {self.factor_weights.get('volatility_60d', 0):.2%}")

        # èµ„é‡‘æµå› å­
        print("  â”œâ”€ èµ„é‡‘æµå› å­ (15%)" if self.use_fundamental else "  â”œâ”€ èµ„é‡‘æµå› å­ (20%)")
        print(f"  â”‚   â”œâ”€ 20æ—¥èµ„é‡‘æµ: {self.factor_weights.get('money_flow_20d', 0):.2%}")
        print(f"  â”‚   â””â”€ é‡æ¯”: {self.factor_weights.get('volume_ratio', 0):.2%}")

        # åŠ¨é‡å› å­
        print("  â”œâ”€ åŠ¨é‡å› å­ (15%)" if self.use_fundamental else "  â””â”€ åŠ¨é‡å› å­ (20%)")
        print(f"  â”‚   â”œâ”€ 20æ—¥æ”¶ç›ŠçŽ‡: {self.factor_weights.get('return_20d', 0):.2%}")
        print(f"  â”‚   â””â”€ 60æ—¥æ”¶ç›ŠçŽ‡: {self.factor_weights.get('return_60d', 0):.2%}")

        # åŸºæœ¬é¢å› å­ (æ–°å¢ž)
        if self.use_fundamental:
            print("  â””â”€ åŸºæœ¬é¢å› å­ (30%) âœ¨æ–°å¢ž")
            print(f"      â”œâ”€ ROE(å‡€èµ„äº§æ”¶ç›ŠçŽ‡): {self.factor_weights.get('roe', 0):.2%}")
            print(f"      â”œâ”€ ROA(æ€»èµ„äº§æ”¶ç›ŠçŽ‡): {self.factor_weights.get('roa', 0):.2%}")
            print(f"      â”œâ”€ æ¯›åˆ©çŽ‡: {self.factor_weights.get('gross_margin', 0):.2%}")
            print(f"      â”œâ”€ å‡€åˆ©çŽ‡: {self.factor_weights.get('net_margin', 0):.2%}")
            print(f"      â””â”€ èµ„äº§è´Ÿå€ºçŽ‡: {self.factor_weights.get('debt_ratio', 0):.2%}")

    def calculate_valuation_factors(self, df):
        """è®¡ç®—ä¼°å€¼å› å­(ç®€åŒ–ç‰ˆ - ä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡ä¼°ç®—)"""
        df['pe_ratio'] = df['close'] / df.groupby('instrument')['close'].transform('mean')
        df['pb_ratio'] = df['close'] / (df.groupby('instrument')['close'].transform('mean') * 0.8)
        df['ps_ratio'] = df['close'] / (df.groupby('instrument')['close'].transform('mean') * 1.2)
        return df

    def calculate_volatility_factors(self, df):
        """è®¡ç®—æ³¢åŠ¨çŽ‡å› å­"""
        df['volatility_20d'] = df.groupby('instrument')['close'].rolling(20).std().reset_index(0, drop=True)
        df['volatility_60d'] = df.groupby('instrument')['close'].rolling(60).std().reset_index(0, drop=True)
        return df

    def calculate_money_flow_factors(self, df):
        """è®¡ç®—èµ„é‡‘æµå› å­"""
        df['money_flow_20d'] = (df['volume'] * df['close']).rolling(20).mean()
        df['volume_ma5'] = df.groupby('instrument')['volume'].rolling(5).mean().reset_index(0, drop=True)
        df['volume_ma20'] = df.groupby('instrument')['volume'].rolling(20).mean().reset_index(0, drop=True)
        df['volume_ratio'] = df['volume_ma5'] / (df['volume_ma20'] + 1e-6)
        return df

    def calculate_momentum_factors(self, df):
        """è®¡ç®—åŠ¨é‡å› å­"""
        df['return_20d'] = df.groupby('instrument')['close'].pct_change(20)
        df['return_60d'] = df.groupby('instrument')['close'].pct_change(60)
        return df

    def process_fundamental_factors(self, df):
        """
        å¤„ç†åŸºæœ¬é¢å› å­
        åŸºæœ¬é¢æ•°æ®å·²é€šè¿‡merge_financial_data_to_dailyåˆå¹¶åˆ°dfä¸­
        è¿™é‡Œåªéœ€ç¡®ä¿æ•°æ®è´¨é‡å’Œå¤„ç†å¼‚å¸¸å€¼
        """
        if not self.use_fundamental:
            return df

        fundamental_cols = ['roe', 'roa', 'gross_margin', 'net_margin', 'debt_ratio']

        for col in fundamental_cols:
            if col in df.columns:
                # å¤„ç†å¼‚å¸¸å€¼:ä½¿ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
                median_val = df.groupby('instrument')[col].transform('median')
                df[col] = df[col].fillna(median_val)

                # é™åˆ¶æžç«¯å€¼(ä½¿ç”¨1%å’Œ99%åˆ†ä½æ•°)
                lower = df[col].quantile(0.01)
                upper = df[col].quantile(0.99)
                df[col] = df[col].clip(lower, upper)

        return df

    def calculate_all_factors(self, price_data):
        """è®¡ç®—æ‰€æœ‰å› å­"""
        print("\nâš™ï¸  è®¡ç®—StockRankerå¤šå› å­...")
        df = price_data.copy()

        print("  â”œâ”€ ä¼°å€¼å› å­...")
        df = self.calculate_valuation_factors(df)

        print("  â”œâ”€ æ³¢åŠ¨çŽ‡å› å­...")
        df = self.calculate_volatility_factors(df)

        print("  â”œâ”€ èµ„é‡‘æµå› å­...")
        df = self.calculate_money_flow_factors(df)

        print("  â”œâ”€ åŠ¨é‡å› å­...")
        df = self.calculate_momentum_factors(df)

        if self.use_fundamental:
            print("  â””â”€ åŸºæœ¬é¢å› å­å¤„ç†...")
            df = self.process_fundamental_factors(df)

        print("âœ“ å› å­è®¡ç®—å®Œæˆ")
        return df

    def normalize_factors(self, df):
        """æ ‡å‡†åŒ–å› å­(æŒ‰æ—¥æœŸæŽ’åºç™¾åˆ†ä½)"""
        for factor in self.factor_weights.keys():
            if factor in df.columns:
                # ä½¿ç”¨rankè¿›è¡Œæ ‡å‡†åŒ–,å¤„ç†ç¼ºå¤±å€¼
                df[f'{factor}_norm'] = df.groupby('date')[factor].rank(pct=True)
        return df

    def calculate_position_score(self, df):
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        print("\nðŸ“Š è®¡ç®—ç»¼åˆè¯„åˆ†...")
        df = self.normalize_factors(df)

        # åŠ æƒæ±‚å’Œ
        df['position'] = 0
        for factor, weight in self.factor_weights.items():
            norm_factor = f'{factor}_norm'
            if norm_factor in df.columns:
                df['position'] += df[norm_factor].fillna(0.5) * weight

        # å½’ä¸€åŒ–åˆ°0-1åŒºé—´
        min_score = df.groupby('date')['position'].transform('min')
        max_score = df.groupby('date')['position'].transform('max')
        df['position'] = (df['position'] - min_score) / (max_score - min_score + 1e-6)

        print("âœ“ è¯„åˆ†è®¡ç®—å®Œæˆ")
        return df


def load_data_from_tushare(start_date, end_date, max_stocks=50, use_cache=True,
                           cache_manager=None, use_stockranker=True,
                           custom_weights=None, tushare_token=None,
                           use_fundamental=True):
    """
    ä»ŽTushareåŠ è½½æ•°æ®å¹¶è®¡ç®—å› å­ (æ‰©å±•ç‰ˆ - æ”¯æŒåŸºæœ¬é¢å› å­ + è¡Œä¸šæ•°æ®)

    :param start_date: å¼€å§‹æ—¥æœŸ
    :param end_date: ç»“æŸæ—¥æœŸ
    :param max_stocks: æœ€å¤§è‚¡ç¥¨æ•°
    :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    :param cache_manager: ç¼“å­˜ç®¡ç†å™¨
    :param use_stockranker: æ˜¯å¦ä½¿ç”¨StockRankeræ¨¡åž‹
    :param custom_weights: è‡ªå®šä¹‰å› å­æƒé‡
    :param tushare_token: Tushare token
    :param use_fundamental: æ˜¯å¦ä½¿ç”¨åŸºæœ¬é¢å› å­
    """
    print("\n" + "=" * 80)
    print("ðŸ“¦ æ•°æ®åŠ è½½æ¨¡å— (Tushareç‰ˆ + åŸºæœ¬é¢ + è¡Œä¸š)")
    print("=" * 80)

    model_type = "StockRankerå¤šå› å­" if use_stockranker else "ç®€å•æŠ€æœ¯å› å­"
    if use_stockranker and use_fundamental:
        model_type += " + åŸºæœ¬é¢"
    print(f"å› å­æ¨¡åž‹: {model_type}")

    # ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
    model_suffix = "stockranker" if use_stockranker else "simple"
    if use_fundamental:
        model_suffix += "_fundamental"

    cache_key = f"factor_data_ts_{start_date}_{end_date}_{max_stocks}_{model_suffix}"
    price_cache_key = f"price_data_ts_{start_date}_{end_date}_{max_stocks}"

    # å°è¯•ä»Žç¼“å­˜åŠ è½½
    if use_cache and cache_manager:
        print("\nðŸ” æ£€æŸ¥ç¼“å­˜...")
        factor_data = cache_manager.load_from_csv(cache_key)
        price_data = cache_manager.load_from_csv(price_cache_key)

        if factor_data is not None and price_data is not None:
            print("âœ“ ä½¿ç”¨ç¼“å­˜æ•°æ®")
            print(f"  - å› å­æ•°æ®: {len(factor_data)} æ¡")
            print(f"  - ä»·æ ¼æ•°æ®: {len(price_data)} æ¡")
            return factor_data, price_data
        else:
            print("âœ— ç¼“å­˜æœªæ‰¾åˆ°,å¼€å§‹ä»ŽTushareèŽ·å–...")

    # åˆå§‹åŒ–Tushareæ•°æ®æº
    data_source = TushareDataSource(
        cache_manager=cache_manager if use_cache else None,
        token=tushare_token
    )

    # èŽ·å–è‚¡ç¥¨åˆ—è¡¨
    stock_list = data_source.get_stock_list()
    if not stock_list:
        print("âœ— æ— æ³•èŽ·å–è‚¡ç¥¨åˆ—è¡¨!")
        return None, None

    stock_list = stock_list[:max_stocks]

    # ========== èŽ·å–ä»·æ ¼æ•°æ® ==========
    all_price_data = []
    success_count = 0

    print(f"\nðŸ“Š èŽ·å– {len(stock_list)} åªè‚¡ç¥¨çš„åŽ†å²æ•°æ®...")
    print("è¿›åº¦: ", end='')

    for i, ts_code in enumerate(stock_list):
        if (i + 1) % 5 == 0:
            print(f"{i + 1}/{len(stock_list)} ", end='', flush=True)

        df = data_source.get_price_data(ts_code, start_date, end_date)
        if df is not None and len(df) > 0:
            all_price_data.append(df)
            success_count += 1

    print(f"\nâœ“ æˆåŠŸèŽ·å– {success_count}/{len(stock_list)} åªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®")

    if len(all_price_data) == 0:
        print("âœ— æœªèŽ·å–åˆ°ä»»ä½•æ•°æ®!")
        return None, None

    # åˆå¹¶ä»·æ ¼æ•°æ®
    price_df = pd.concat(all_price_data, ignore_index=True)

    # ========== èŽ·å–åŸºæœ¬é¢æ•°æ® ==========
    if use_stockranker and use_fundamental:
        print(f"\nðŸ“ˆ èŽ·å–åŸºæœ¬é¢è´¢åŠ¡æ•°æ®...")
        all_financial_data = []
        financial_success = 0

        print("è¿›åº¦: ", end='')
        for i, ts_code in enumerate(stock_list):
            if (i + 1) % 5 == 0:
                print(f"{i + 1}/{len(stock_list)} ", end='', flush=True)

            financial_df = data_source.get_financial_indicators(ts_code, start_date, end_date)
            if financial_df is not None and len(financial_df) > 0:
                all_financial_data.append(financial_df)
                financial_success += 1

        print(f"\nâœ“ æˆåŠŸèŽ·å– {financial_success}/{len(stock_list)} åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®")

        if len(all_financial_data) > 0:
            financial_df = pd.concat(all_financial_data, ignore_index=True)
            print("\nðŸ”— åˆå¹¶åŸºæœ¬é¢æ•°æ®åˆ°æ—¥çº¿æ•°æ®...")
            price_df = data_source.merge_financial_data_to_daily(price_df, financial_df)
            print("âœ“ åŸºæœ¬é¢æ•°æ®åˆå¹¶å®Œæˆ")

            fundamental_cols = ['roe', 'roa', 'gross_margin', 'net_margin', 'debt_ratio']
            available_cols = [col for col in fundamental_cols if col in price_df.columns]
            if available_cols:
                coverage = (price_df[available_cols].notna().any(axis=1).sum() / len(price_df)) * 100
                print(f"  åŸºæœ¬é¢æ•°æ®è¦†ç›–çŽ‡: {coverage:.1f}%")
        else:
            print("âš ï¸  æœªèŽ·å–åˆ°åŸºæœ¬é¢æ•°æ®,å°†ä¸ä½¿ç”¨åŸºæœ¬é¢å› å­")
            use_fundamental = False

    price_df['date'] = price_df['date'].astype(str)

    # ========== é€‰æ‹©å› å­è®¡ç®—æ–¹æ³• ==========
    if use_stockranker:
        model = StockRankerModel(
            custom_weights=custom_weights,
            use_fundamental=use_fundamental
        )
        factor_df = model.calculate_all_factors(price_df)
        factor_df = model.calculate_position_score(factor_df)
    else:
        print("\nâš™ï¸  è®¡ç®—ç®€å•æŠ€æœ¯å› å­...")
        factor_df = calculate_simple_factors(price_df)

    factor_df = factor_df.dropna(subset=['position'])

    result_factor = factor_df[['date', 'instrument', 'position']].copy()
    result_price = price_df.copy()

    # ========== âœ… å…³é”®æ·»åŠ ï¼šèŽ·å–å¹¶åˆå¹¶è¡Œä¸šæ•°æ® ==========
    print("\nðŸ“Š èŽ·å–è¡Œä¸šæ•°æ®...")
    industry_data = data_source.get_industry_data(stock_list, use_cache=use_cache)

    if industry_data is not None and len(industry_data) > 0:
        # åˆå¹¶è¡Œä¸šæ•°æ®åˆ°å› å­æ•°æ®
        result_factor = result_factor.merge(
            industry_data,
            on='instrument',
            how='left'
        )
        result_factor['industry'] = result_factor['industry'].fillna('å…¶ä»–')
        print(f"  âœ“ è¡Œä¸šæ•°æ®å·²åˆå¹¶åˆ°å› å­æ•°æ®")
    else:
        result_factor['industry'] = 'Unknown'
        print(f"  âš ï¸  æœªèƒ½èŽ·å–è¡Œä¸šæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼")

    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache and cache_manager:
        print("\nðŸ’¾ ä¿å­˜åˆ°ç¼“å­˜...")
        cache_manager.save_to_csv(result_factor, cache_key)
        cache_manager.save_to_csv(result_price, price_cache_key)

    print(f"\nâœ“ æ•°æ®å‡†å¤‡å®Œæˆ:")
    print(f"  - å› å­æ•°æ®: {len(result_factor)} æ¡")
    print(f"  - ä»·æ ¼æ•°æ®: {len(result_price)} æ¡")
    print(f"  - è‚¡ç¥¨æ•°é‡: {result_factor['instrument'].nunique()} åª")
    print(f"  - äº¤æ˜“æ—¥æ•°: {result_factor['date'].nunique()} å¤©")
    print(f"  - è¡Œä¸šæ•°é‡: {result_factor['industry'].nunique()} ä¸ª")  # âœ… æ·»åŠ è¡Œä¸šç»Ÿè®¡

    if use_fundamental and use_stockranker:
        print(f"  - åŸºæœ¬é¢å› å­: å·²å¯ç”¨ (ROE/ROA/æ¯›åˆ©çŽ‡/å‡€åˆ©çŽ‡/èµ„äº§è´Ÿå€ºçŽ‡)")

    return result_factor, result_price

def calculate_simple_factors(price_data):
    """è®¡ç®—ç®€å•æŠ€æœ¯å› å­(å…¼å®¹æ—§ç‰ˆæœ¬)"""
    df = price_data.copy()

    # åŠ¨é‡å› å­
    df['return_5d'] = df.groupby('instrument')['close'].pct_change(5)
    df['return_10d'] = df.groupby('instrument')['close'].pct_change(10)
    df['return_20d'] = df.groupby('instrument')['close'].pct_change(20)

    # æ³¢åŠ¨çŽ‡å› å­
    df['volatility_20d'] = df.groupby('instrument')['close'].rolling(20).std().reset_index(0, drop=True)

    # æˆäº¤é‡å› å­
    df['volume_ma5'] = df.groupby('instrument')['volume'].rolling(5).mean().reset_index(0, drop=True)
    df['volume_ma20'] = df.groupby('instrument')['volume'].rolling(20).mean().reset_index(0, drop=True)
    df['volume_ratio'] = df['volume_ma5'] / (df['volume_ma20'] + 1e-6)

    # RSIå› å­
    delta = df.groupby('instrument')['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / (loss + 1e-6)
    df['rsi'] = 100 - (100 / (1 + rs))

    # æ ‡å‡†åŒ–å„å› å­
    for col in ['return_20d', 'volume_ratio', 'rsi']:
        if col in df.columns:
            df[f'{col}_norm'] = df.groupby('date')[col].rank(pct=True)

    # ç»¼åˆè¯„åˆ†
    weights = {
        'return_20d_norm': 0.4,
        'volume_ratio_norm': 0.3,
        'rsi_norm': 0.3
    }

    df['position'] = 0
    for factor, weight in weights.items():
        if factor in df.columns:
            df['position'] += df[factor].fillna(0.5) * weight

    df['position'] = df['position'] - (df['rsi_norm'].fillna(0.5) - 0.5) * 0.2

    return df

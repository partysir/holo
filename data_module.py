"""
data_module.py - æ•°æ®ç®¡ç†æ¨¡å— (å®Œæ•´ä¿®å¤ç‰ˆ v2.6)
ä¿®å¤ Tushare API é™æµé—®é¢˜ + ä¿ç•™æ‰€æœ‰åŸæœ‰åŠŸèƒ½ + ä¿®å¤åˆ—ç´¢å¼•é”™è¯¯ + è¿‡æ»¤STè‚¡ç¥¨

ä¸»è¦æ”¹è¿›:
âœ… ä¿®å¤ KeyError: "['position', 'amount'] not in index"
âœ… get_price_data è¿”å› amount åˆ—
âœ… load_data_from_tushare æ­£ç¡®åˆ†ç¦»ä»·æ ¼åˆ—å’Œå› å­åˆ—
âœ… æ™ºèƒ½é™æµæ§åˆ¶ (è‡ªé€‚åº”ç­‰å¾…)
âœ… æ‰¹é‡è¯·æ±‚ä¼˜åŒ– (å‡å°‘APIè°ƒç”¨æ¬¡æ•°)
âœ… æ–°å¢: è‡ªåŠ¨è¿‡æ»¤ ST/S*ST/*ST è‚¡ç¥¨
"""

import pandas as pd
import numpy as np
import os
import pickle
import hashlib
import time
from datetime import datetime, timedelta
from collections import deque

# Tushareå¯¼å…¥
try:
    import tushare as ts
    TUSHARE_AVAILABLE = True
except ImportError:
    TUSHARE_AVAILABLE = False
    print("âš ï¸  Tushareæœªå®‰è£…: pip install tushare")

# å¯¼å…¥èµ„é‡‘æµå› å­è®¡ç®—å™¨
from money_flow_factors import MoneyFlowFactorCalculator, integrate_money_flow_to_stockranker


# ========== ç¬¬1éƒ¨åˆ†ï¼šåŸºç¡€å·¥å…·ç±» ==========

class DataCache:
    """æ•°æ®ç¼“å­˜ç®¡ç†ç±»"""

    def __init__(self, cache_dir='./data_cache'):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        self.cache_dir = cache_dir
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
            print(f"âœ“ åˆ›å»ºç¼“å­˜ç›®å½•: {cache_dir}")

    def _get_cache_key(self, prefix, start_date, end_date, **kwargs):
        """ç”Ÿæˆç¼“å­˜key"""
        key_str = f"{prefix}_{start_date}_{end_date}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(key_str.encode()).hexdigest()

    def save_to_cache(self, data, cache_name):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜ (Pickle)"""
        cache_path = os.path.join(self.cache_dir, f"{cache_name}.pkl")
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
            return True
        except Exception as e:
            print(f"âœ— ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False

    def load_from_cache(self, cache_name):
        """ä»ç¼“å­˜åŠ è½½æ•°æ® (Pickle)"""
        cache_path = os.path.join(self.cache_dir, f"{cache_name}.pkl")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    data = pickle.load(f)
                return data
            except Exception as e:
                print(f"âœ— ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        return None

    def save_to_csv(self, df, filename):
        """ä¿å­˜DataFrameåˆ°CSV"""
        csv_path = os.path.join(self.cache_dir, f"{filename}.csv")
        try:
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"âœ“ CSVå·²ä¿å­˜: {filename}")
            return True
        except Exception as e:
            print(f"âœ— CSVä¿å­˜å¤±è´¥: {e}")
            return False

    def load_from_csv(self, filename):
        """ä»CSVåŠ è½½DataFrame"""
        csv_path = os.path.join(self.cache_dir, f"{filename}.csv")
        if os.path.exists(csv_path):
            try:
                df = pd.read_csv(csv_path, encoding='utf-8-sig')
                print(f"âœ“ ä»CSVåŠ è½½: {filename}")
                return df
            except Exception as e:
                print(f"âœ— CSVåŠ è½½å¤±è´¥: {e}")
        return None

    def list_cache_files(self):
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
        if not os.path.exists(self.cache_dir):
            return []
        files = []
        for f in os.listdir(self.cache_dir):
            if f.endswith('.pkl') or f.endswith('.csv'):
                file_path = os.path.join(self.cache_dir, f)
                file_size = os.path.getsize(file_path) / 1024  # KB
                file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                files.append({
                    'name': f,
                    'size_kb': f"{file_size:.2f}",
                    'modified': file_time.strftime('%Y-%m-%d %H:%M:%S')
                })
        return files

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if os.path.exists(self.cache_dir):
            count = 0
            for f in os.listdir(self.cache_dir):
                file_path = os.path.join(self.cache_dir, f)
                try:
                    os.remove(file_path)
                    count += 1
                except Exception as e:
                    print(f"âœ— åˆ é™¤æ–‡ä»¶å¤±è´¥ {f}: {e}")
            print(f"âœ“ å·²æ¸…ç©ºç¼“å­˜ ({count}ä¸ªæ–‡ä»¶)")


class RateLimiter:
    """è®¿é—®é¢‘ç‡æ§åˆ¶å™¨ - æ¯åˆ†é’Ÿ800æ¬¡è®¿é—®é™åˆ¶åæš‚åœç­‰å¾…"""

    def __init__(self, max_calls=800, time_window=60):
        """
        åˆå§‹åŒ–é™æµå™¨
        Args:
            max_calls: æ—¶é—´çª—å£å†…æœ€å¤§è°ƒç”¨æ¬¡æ•° (é»˜è®¤800/åˆ†é’Ÿ)
            time_window: æ—¶é—´çª—å£(ç§’)
        """
        self.max_calls = max_calls
        self.time_window = time_window
        self.call_times = deque()  # è®°å½•è°ƒç”¨æ—¶é—´æˆ³
        self.total_calls = 0
        self.total_waits = 0

    def wait_if_needed(self):
        """ç­‰å¾…ç›´åˆ°å¯ä»¥ç»§ç»­è°ƒç”¨API - ç¡®ä¿ä¸è¶…è¿‡é¢‘ç‡é™åˆ¶"""
        now = time.time()

        # ç§»é™¤æ—¶é—´çª—å£å¤–çš„è®°å½•
        while self.call_times and now - self.call_times[0] > self.time_window:
            self.call_times.popleft()

        # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œç­‰å¾…åˆ°æœ€æ—©çš„è°ƒç”¨è¶…å‡ºæ—¶é—´çª—å£
        while len(self.call_times) >= self.max_calls:
            sleep_time = self.time_window - (now - self.call_times[0]) + 0.1
            if sleep_time > 0:
                self.total_waits += 1
                print(f"â³ è§¦å‘è®¿é—®é™åˆ¶ï¼Œç­‰å¾… {sleep_time:.1f} ç§’...")
                time.sleep(sleep_time)
                now = time.time()
                # æ¸…ç†è¿‡æœŸè®°å½•
                while self.call_times and now - self.call_times[0] > self.time_window:
                    self.call_times.popleft()
            else:
                break

        # è®°å½•æœ¬æ¬¡è°ƒç”¨
        self.call_times.append(now)
        self.total_calls += 1

        # åŸºç¡€å»¶è¿Ÿ(é¿å…ç¬æ—¶é«˜å³°)
        time.sleep(0.05)

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_calls': self.total_calls,
            'total_waits': self.total_waits,
            'current_window_calls': len(self.call_times)
        }


# ========== ç¬¬2éƒ¨åˆ†ï¼šTushareæ•°æ®æºç±» ==========

class TushareDataSource:
    """Tushareæ•°æ®æºç®¡ç†ç±» - ä¼˜åŒ–é™æµç‰ˆæœ¬"""

    def __init__(self, cache_manager=None, token=None, rate_limiter=None):
        """åˆå§‹åŒ–Tushareæ•°æ®æº"""
        self.cache = cache_manager

        if not TUSHARE_AVAILABLE:
            raise ImportError("è¯·å…ˆå®‰è£…Tushare: pip install tushare")

        if token:
            ts.set_token(token)

        try:
            self.pro = ts.pro_api()
            print("âœ“ Tushare APIåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âœ— Tushareåˆå§‹åŒ–å¤±è´¥: {e}")
            print("è¯·è®¾ç½®token: ts.set_token('ä½ çš„token')")
            self.pro = None

        # åˆå§‹åŒ–é™æµå™¨
        self.rate_limiter = rate_limiter or RateLimiter(max_calls=800, time_window=60)
        print(f"âœ“ é™æµå™¨å·²å¯ç”¨: {self.rate_limiter.max_calls}æ¬¡/åˆ†é’Ÿ")

    def get_stock_list(self, date=None, min_days_listed=180):
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨ (ä¿®å¤ç‰ˆ - å¢åŠ ä¸Šå¸‚æ—¥æœŸè¿‡æ»¤å’ŒSTè¿‡æ»¤)
        """
        if self.pro is None:
            return []

        try:
            print("ä½¿ç”¨Tushareè·å–è‚¡ç¥¨åˆ—è¡¨...")
            self.rate_limiter.wait_if_needed()

            # ç¡®ä¿ fields ä¸­åŒ…å« 'name' ä»¥ä¾¿è¿‡æ»¤ ST
            df = self.pro.stock_basic(
                exchange='',
                list_status='L',
                fields='ts_code,symbol,name,area,industry,market,list_date'
            )

            # ========== å…³é”®ä¿®å¤ Issue A: è¿‡æ»¤ä¸Šå¸‚æ—¥æœŸ ==========
            if date:
                backtest_start = pd.to_datetime(date)
                latest_list_date = backtest_start - timedelta(days=min_days_listed)
                df['list_date'] = pd.to_datetime(df['list_date'], format='%Y%m%d', errors='coerce')

                original_count = len(df)
                df = df[df['list_date'] <= latest_list_date].copy()
                filtered_count = original_count - len(df)

                print(f"  ğŸ“… ä¸Šå¸‚æ—¥æœŸè¿‡æ»¤: å›æµ‹å¼€å§‹ {date}, è¿‡æ»¤æ–°è‚¡ {filtered_count} åª")

            # ========== å…³é”®ä¿®å¤: è¿‡æ»¤ ST è‚¡ç¥¨ ==========
            if 'name' in df.columns:
                original_count = len(df)
                df = df[~df['name'].str.contains('ST', case=False, na=False)].copy()
                st_filtered = original_count - len(df)
                print(f"  ğŸ—‘ï¸ STè‚¡ç¥¨è¿‡æ»¤: å‰”é™¤ {st_filtered} åªé£é™©è­¦ç¤ºè‚¡")

            # è¿‡æ»¤ç‰¹æ®Šæ¿å—
            original_count = len(df)
            df = df[~df['symbol'].str.startswith(('688', '300', '8', '4', '92'))].copy()
            special_filtered = original_count - len(df)

            if special_filtered > 0:
                print(f"  ğŸš« ç‰¹æ®Šæ¿å—è¿‡æ»¤: {special_filtered} åª (ç§‘åˆ›æ¿/åˆ›ä¸šæ¿/åŒ—äº¤æ‰€)")

            stock_codes = df['ts_code'].tolist()
            print(f"âœ“ æœ€ç»ˆè·å– {len(stock_codes)} åªç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨")

            return stock_codes

        except Exception as e:
            print(f"âœ— è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def get_price_data(self, ts_code, start_date, end_date, list_date=None, max_retries=3):
        """è·å–å•åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ® (å¸¦é™æµå’Œé‡è¯•)"""
        if self.pro is None:
            return None

        cache_name = f"price_{ts_code}_v2.5_{start_date.replace('-', '')}_{end_date.replace('-', '')}"

        if self.cache:
            cached_data = self.cache.load_from_cache(cache_name)
            if cached_data is not None:
                return cached_data

        for attempt in range(max_retries):
            try:
                self.rate_limiter.wait_if_needed()
                df = self.pro.daily(
                    ts_code=ts_code,
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', ''),
                    fields='trade_date,open,high,low,close,vol,amount'
                )

                if df is None or len(df) == 0:
                    return None

                df = df.rename(columns={'trade_date': 'date', 'vol': 'volume'})
                df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
                df['instrument'] = ts_code
                df = df.sort_values('date').reset_index(drop=True)

                if list_date is not None:
                    list_date_dt = pd.to_datetime(list_date, format='%Y%m%d', errors='coerce')
                    if pd.notna(list_date_dt):
                        df = df[df['date'] >= list_date_dt].copy()

                # âœ… ä¿®å¤: æ·»åŠ  amount åˆ—
                result = df[['date', 'instrument', 'open', 'close', 'high', 'low', 'volume', 'amount']]

                if self.cache:
                    self.cache.save_to_cache(result, cache_name)

                return result

            except Exception as e:
                error_msg = str(e)
                if "æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®" in error_msg or "æŠ±æ­‰" in error_msg:
                    wait_time = 5 * (attempt + 1)
                    print(f"    â³ {ts_code}: è§¦å‘é™æµï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•... ({attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    print(f"    âœ— {ts_code}: {e}")
                    break

        return None

    def get_index_daily(self, ts_code='000001.SH', start_date=None, end_date=None):
        """è·å–æŒ‡æ•°æ—¥çº¿æ•°æ® (ç”¨äºæ‹©æ—¶)"""
        if self.pro is None:
            return None

        cache_name = f"index_{ts_code}_{start_date}_{end_date}"
        if self.cache:
            cached_data = self.cache.load_from_cache(cache_name)
            if cached_data is not None:
                return cached_data

        try:
            print(f"  ğŸ“Š è·å–æŒ‡æ•°æ•°æ®: {ts_code}...")
            self.rate_limiter.wait_if_needed()

            df = self.pro.index_daily(
                ts_code=ts_code,
                start_date=start_date.replace('-', '') if start_date else None,
                end_date=end_date.replace('-', '') if end_date else None,
                fields='trade_date,close,open,high,low,vol'
            )

            if df is None or len(df) == 0:
                return None

            df = df.rename(columns={'trade_date': 'date', 'vol': 'volume'})
            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
            df = df.sort_values('date').reset_index(drop=True)
            df['date'] = df['date'].astype(str)

            if self.cache:
                self.cache.save_to_cache(df, cache_name)

            return df

        except Exception as e:
            print(f"  âš ï¸  è·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e}")
            return None

    def get_daily_basic(self, ts_code, start_date, end_date):
        """è·å–æ¯æ—¥æŒ‡æ ‡æ•°æ®(PE/PB/PSç­‰)"""
        if self.pro is None:
            return None

        try:
            self.rate_limiter.wait_if_needed()
            df = self.pro.daily_basic(
                ts_code=ts_code,
                start_date=start_date.replace('-', ''),
                end_date=end_date.replace('-', ''),
                fields='trade_date,pe,pb,ps,total_mv'
            )

            if df is None or len(df) == 0:
                return None

            df = df.rename(columns={'trade_date': 'date'})
            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
            df['instrument'] = ts_code
            return df

        except Exception as e:
            print(f"âœ— è·å– {ts_code} åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
            return None

    def get_financial_indicators(self, ts_code, start_date, end_date, max_retries=3):
        """è·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ®(ROE, ROA, æ¯›åˆ©ç‡, å‡€åˆ©ç‡, èµ„äº§è´Ÿå€ºç‡)"""
        if self.pro is None:
            return None

        cache_name = f"financial_{ts_code}_{start_date}_{end_date}"
        if self.cache:
            cached_data = self.cache.load_from_cache(cache_name)
            if cached_data is not None:
                return cached_data

        for attempt in range(max_retries):
            try:
                self.rate_limiter.wait_if_needed()
                df = self.pro.fina_indicator(
                    ts_code=ts_code,
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', ''),
                    fields='ts_code,ann_date,end_date,roe,roa,grossprofit_margin,netprofit_margin,debt_to_assets'
                )

                if df is None or len(df) == 0:
                    return None

                df = df.rename(columns={
                    'ann_date': 'date',
                    'grossprofit_margin': 'gross_margin',
                    'netprofit_margin': 'net_margin',
                    'debt_to_assets': 'debt_ratio'
                })

                df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
                df['instrument'] = ts_code
                df = df.sort_values('date').reset_index(drop=True)

                if self.cache:
                    self.cache.save_to_cache(df, cache_name)
                return df

            except Exception as e:
                error_msg = str(e)
                if "æ¯åˆ†é’Ÿæœ€å¤šè®¿é—®" in error_msg or "æŠ±æ­‰" in error_msg:
                    wait_time = 5 * (attempt + 1)
                    print(f"    â³ {ts_code}: è§¦å‘é™æµï¼Œç­‰å¾…{wait_time}ç§’... ({attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                else:
                    print(f"âœ— è·å– {ts_code} è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {e}")
                    break
        return None

    def get_industry_data(self, instruments, use_cache=True):
        """è·å–è‚¡ç¥¨è¡Œä¸šæ•°æ®"""
        if self.pro is None:
            return pd.DataFrame({'instrument': instruments, 'industry': 'Unknown'})

        cache_name = "industry_data_all_v2.5"
        if use_cache and self.cache:
            cached_data = self.cache.load_from_cache(cache_name)
            if cached_data is not None:
                cached_data = cached_data[cached_data['instrument'].isin(instruments)]
                if len(cached_data) > 0:
                    print(f"  âœ“ ä»ç¼“å­˜åŠ è½½è¡Œä¸šæ•°æ®")
                    return cached_data

        try:
            print(f"  ğŸ“Š è·å– {len(instruments)} åªè‚¡ç¥¨çš„è¡Œä¸šæ•°æ®...")
            self.rate_limiter.wait_if_needed()

            stock_basic = self.pro.stock_basic(exchange='', list_status='L', fields='ts_code,name,industry')

            if use_cache and self.cache:
                stock_basic_cache = stock_basic.rename(columns={'ts_code': 'instrument'})
                self.cache.save_to_cache(stock_basic_cache[['instrument', 'industry']], cache_name)

            stock_basic = stock_basic[stock_basic['ts_code'].isin(instruments)]
            stock_basic = stock_basic.rename(columns={'ts_code': 'instrument'})
            stock_basic['industry'] = stock_basic['industry'].fillna('å…¶ä»–')
            result = stock_basic[['instrument', 'industry']]

            missing = set(instruments) - set(result['instrument'])
            if missing:
                missing_df = pd.DataFrame({'instrument': list(missing), 'industry': 'å…¶ä»–'})
                result = pd.concat([result, missing_df], ignore_index=True)

            print(f"  âœ“ è¡Œä¸šæ•°æ®è·å–å®Œæˆ, è¡Œä¸šæ•°: {result['industry'].nunique()}ä¸ª")
            return result

        except Exception as e:
            print(f"  âš ï¸  è·å–è¡Œä¸šæ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame({'instrument': instruments, 'industry': 'Unknown'})

    def merge_financial_data_to_daily(self, price_df, financial_df):
        """å°†å­£åº¦è´¢åŠ¡æ•°æ®åˆå¹¶åˆ°æ—¥çº¿æ•°æ® (Merge Asof)"""
        if financial_df is None or len(financial_df) == 0:
            print("  âš ï¸  è´¢åŠ¡æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆå¹¶")
            return price_df

        price_df['date'] = pd.to_datetime(price_df['date'], errors='coerce')
        financial_df['date'] = pd.to_datetime(financial_df['date'], errors='coerce')

        financial_df = financial_df.dropna(subset=['date', 'instrument'])
        price_df = price_df.dropna(subset=['date', 'instrument'])

        if len(financial_df) == 0:
            return price_df

        result_list = []
        success_count = 0

        print("  ğŸ”— åˆå¹¶è´¢åŠ¡æ•°æ®åˆ°æ—¥çº¿...")
        instruments = price_df['instrument'].unique()

        for idx, instrument in enumerate(instruments):
            if (idx + 1) % 500 == 0:
                print(f"     è¿›åº¦: {idx + 1}/{len(instruments)}")

            price_subset = price_df[price_df['instrument'] == instrument].copy()
            financial_subset = financial_df[financial_df['instrument'] == instrument].copy()

            if len(financial_subset) == 0:
                result_list.append(price_subset)
                continue

            financial_subset = financial_subset.dropna(subset=['date'])

            try:
                merged = pd.merge_asof(
                    price_subset.sort_values('date'),
                    financial_subset.sort_values('date')[
                        ['date', 'roe', 'roa', 'gross_margin', 'net_margin', 'debt_ratio']
                    ],
                    on='date',
                    direction='backward'
                )
                result_list.append(merged)
                success_count += 1
            except Exception:
                result_list.append(price_subset)

        if len(result_list) == 0:
            return price_df

        result_df = pd.concat(result_list, ignore_index=True)
        print(f"  âœ“ åˆå¹¶å®Œæˆ: æˆåŠŸ {success_count} åª")
        return result_df

    def print_rate_limit_stats(self):
        """æ‰“å°é™æµç»Ÿè®¡ä¿¡æ¯"""
        stats = self.rate_limiter.get_stats()
        print(f"\nğŸ“Š APIè°ƒç”¨ç»Ÿè®¡:")
        print(f"  - æ€»è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}")
        print(f"  - è§¦å‘é™æµæ¬¡æ•°: {stats['total_waits']}")


# ========== ç¬¬3éƒ¨åˆ†ï¼šStockRanker å¤šå› å­è¯„åˆ†æ¨¡å‹ ==========

class StockRankerModel:
    """StockRanker å¤šå› å­è¯„åˆ†æ¨¡å‹ (å†…å­˜ä¼˜åŒ–ç‰ˆ)"""

    def __init__(self, custom_weights=None, use_fundamental=True, use_money_flow=True, money_flow_style='balanced'):
        self.use_fundamental = use_fundamental
        self.use_money_flow = use_money_flow
        self.money_flow_style = money_flow_style
        
        # åˆå§‹åŒ–èµ„é‡‘æµè®¡ç®—å™¨
        if self.use_money_flow:
            from money_flow_factors import MoneyFlowFactorCalculator
            self.money_flow_calculator = MoneyFlowFactorCalculator(
                use_full_tick_data=False,
                keep_only_essential=True  # âœ… å…³é”®ï¼šä»…ä¿ç•™æ ¸å¿ƒå› å­
            )
            
            # è·å–æ¨èçš„èµ„é‡‘æµå› å­æƒé‡
            money_flow_weights = self.money_flow_calculator.get_recommended_weights(money_flow_style)
        else:
            money_flow_weights = {}
        
        if custom_weights:
            self.factor_weights = custom_weights
        else:
            # åŸºç¡€å› å­æƒé‡ï¼ˆæ ¹æ®æ˜¯å¦å¯ç”¨èµ„é‡‘æµè°ƒæ•´ï¼‰
            base_weights = {}
            
            if use_fundamental and use_money_flow:
                # åŸºæœ¬é¢ + èµ„é‡‘æµæ¨¡å¼ï¼ˆæ¨èï¼‰
                base_weights = {
                    # ä¼°å€¼å› å­ï¼ˆæƒé‡ä»25%é™åˆ°15%ï¼‰
                    'pe_ratio': -0.06, 'pb_ratio': -0.06, 'ps_ratio': -0.03,
                    
                    # æ³¢åŠ¨ç‡ï¼ˆæƒé‡ä»15%é™åˆ°10%ï¼‰
                    'volatility_20d': -0.05, 'volatility_60d': -0.05,
                    
                    # æˆäº¤é‡ï¼ˆæƒé‡ä»15%é™åˆ°10%ï¼‰
                    'money_flow_20d': 0.05, 'volume_ratio': 0.05,
                    
                    # åŠ¨é‡ï¼ˆæƒé‡ä»15%é™åˆ°12%ï¼‰
                    'return_20d': 0.06, 'return_60d': 0.06,
                    
                    # åŸºæœ¬é¢ï¼ˆæƒé‡ä»30%é™åˆ°25%ï¼‰
                    'roe': 0.08, 'roa': 0.04,
                    'gross_margin': 0.04, 'net_margin': 0.04,
                    'debt_ratio': -0.05,
                }
                # èµ„é‡‘æµæƒé‡ï¼ˆ28%ï¼Œä»money_flow_weightsè·å–ï¼‰
                base_weights.update(money_flow_weights)
                
            elif use_fundamental:
                # ä»…åŸºæœ¬é¢æ¨¡å¼ï¼ˆåŸæœ‰æƒé‡ï¼‰
                base_weights = {
                    'pe_ratio': -0.10, 'pb_ratio': -0.10, 'ps_ratio': -0.05,
                    'volatility_20d': -0.08, 'volatility_60d': -0.07,
                    'money_flow_20d': 0.08, 'volume_ratio': 0.07,
                    'return_20d': 0.08, 'return_60d': 0.07,
                    'roe': 0.10, 'roa': 0.05,
                    'gross_margin': 0.05, 'net_margin': 0.05,
                    'debt_ratio': -0.05
                }
                
            elif use_money_flow:
                # æŠ€æœ¯ + èµ„é‡‘æµæ¨¡å¼
                base_weights = {
                    'pe_ratio': -0.10, 'pb_ratio': -0.10, 'ps_ratio': -0.08,
                    'volatility_20d': -0.08, 'volatility_60d': -0.07,
                    'money_flow_20d': 0.06, 'volume_ratio': 0.06,
                    'return_20d': 0.08, 'return_60d': 0.07,
                }
                base_weights.update(money_flow_weights)
                
            else:
                # ä»…æŠ€æœ¯å› å­æ¨¡å¼ï¼ˆåŸæœ‰æƒé‡ï¼‰
                base_weights = {
                    'pe_ratio': -0.15, 'pb_ratio': -0.15, 'ps_ratio': -0.10,
                    'volatility_20d': -0.10, 'volatility_60d': -0.10,
                    'money_flow_20d': 0.10, 'volume_ratio': 0.10,
                    'return_20d': 0.10, 'return_60d': 0.10
                }
            
            self.factor_weights = base_weights

        print(f"\nğŸ“Š StockRanker æ¨¡å‹åˆå§‹åŒ–")
        print(f"   åŸºæœ¬é¢: {'âœ“' if use_fundamental else 'âœ—'}")
        print(f"   èµ„é‡‘æµ: {'âœ“' if use_money_flow else 'âœ—'}")
        if use_money_flow:
            print(f"   èµ„é‡‘æµé£æ ¼: {money_flow_style}")
            print(f"   å› å­æ•°é‡: {len(self.factor_weights)} ä¸ª")

    def calculate_valuation_factors(self, df):
        df['pe_ratio'] = df['close'] / df.groupby('instrument')['close'].transform('mean')
        df['pb_ratio'] = df['close'] / (df.groupby('instrument')['close'].transform('mean') * 0.8)
        df['ps_ratio'] = df['close'] / (df.groupby('instrument')['close'].transform('mean') * 1.2)
        return df

    def calculate_volatility_factors(self, df):
        df['volatility_20d'] = df.groupby('instrument')['close'].rolling(20).std().reset_index(0, drop=True)
        df['volatility_60d'] = df.groupby('instrument')['close'].rolling(60).std().reset_index(0, drop=True)
        return df

    def calculate_money_flow_factors(self, df):
        df['money_flow_20d'] = (df['volume'] * df['close']).rolling(20).mean()
        df['volume_ma5'] = df.groupby('instrument')['volume'].rolling(5).mean().reset_index(0, drop=True)
        df['volume_ma20'] = df.groupby('instrument')['volume'].rolling(20).mean().reset_index(0, drop=True)
        df['volume_ratio'] = df['volume_ma5'] / (df['volume_ma20'] + 1e-6)
        return df

    def calculate_momentum_factors(self, df):
        df['return_20d'] = df.groupby('instrument')['close'].pct_change(20)
        df['return_60d'] = df.groupby('instrument')['close'].pct_change(60)
        return df

    def process_fundamental_factors(self, df):
        if not self.use_fundamental: return df
        fundamental_cols = ['roe', 'roa', 'gross_margin', 'net_margin', 'debt_ratio']
        for col in fundamental_cols:
            if col in df.columns:
                median_val = df.groupby('instrument')[col].transform('median')
                df[col] = df[col].fillna(median_val)
                lower = df[col].quantile(0.01)
                upper = df[col].quantile(0.99)
                df[col] = df[col].clip(lower, upper)
        return df

    def calculate_all_factors(self, price_data):
        print("\nâš™ï¸  è®¡ç®—StockRankerå¤šå› å­...")
        df = price_data.copy()
        
        # åŸæœ‰å› å­è®¡ç®—
        df = self.calculate_valuation_factors(df)
        df = self.calculate_volatility_factors(df)
        df = self.calculate_money_flow_factors(df)
        df = self.calculate_momentum_factors(df)
        
        if self.use_fundamental:
            df = self.process_fundamental_factors(df)
        
        # âœ… èµ„é‡‘æµå› å­è®¡ç®—ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
        if self.use_money_flow:
            print("\nğŸ’° è®¡ç®—èµ„é‡‘æµå› å­...")
            df = self.money_flow_calculator.calculate_simplified_money_flow(df)
            
            # æ‰“å°æ‘˜è¦ï¼ˆåŒ…å«å†…å­˜å ç”¨ï¼‰
            self.money_flow_calculator.print_factor_summary(df)
        
        return df

    def normalize_factors(self, df):
        for factor in self.factor_weights.keys():
            if factor in df.columns:
                df[f'{factor}_norm'] = df.groupby('date')[factor].rank(pct=True)
        return df

    def calculate_position_score(self, df):
        print("\nğŸ“Š è®¡ç®—ç»¼åˆè¯„åˆ†...")
        
        # âœ… å…³é”®ä¼˜åŒ–ï¼šé¿å…ä¸€æ¬¡æ€§æ ‡å‡†åŒ–æ‰€æœ‰å› å­
        # åˆ†æ‰¹æ ‡å‡†åŒ–ï¼Œç«‹å³è®¡ç®—è´¡çŒ®
        
        df['position'] = 0.0
        
        for factor, weight in self.factor_weights.items():
            if factor in df.columns:
                # ç›´æ¥æ ‡å‡†åŒ–å¹¶ç´¯åŠ ï¼Œä¸ä¿ç•™ _norm åˆ—
                factor_rank = df.groupby('date')[factor].rank(pct=True).fillna(0.5)
                df['position'] += factor_rank * weight
                
                # ç«‹å³åˆ é™¤ä¸´æ—¶å˜é‡
                del factor_rank
        
        # å½’ä¸€åŒ–åˆ°0-1
        min_score = df.groupby('date')['position'].transform('min')
        max_score = df.groupby('date')['position'].transform('max')
        df['position'] = (df['position'] - min_score) / (max_score - min_score + 1e-6)
        
        # æ¸…ç†
        del min_score, max_score
        
        print("âœ“ è¯„åˆ†è®¡ç®—å®Œæˆ")
        return df


# ========== ç¬¬4éƒ¨åˆ†ï¼šç®€å•å› å­è®¡ç®—å‡½æ•° ==========

def calculate_simple_factors(price_data):
    """è®¡ç®—ç®€å•æŠ€æœ¯å› å­(å…¼å®¹æ—§ç‰ˆæœ¬)"""
    df = price_data.copy()

    # åŠ¨é‡å› å­
    df['return_5d'] = df.groupby('instrument')['close'].pct_change(5)
    df['return_20d'] = df.groupby('instrument')['close'].pct_change(20)

    # æ³¢åŠ¨ç‡ä¸æˆäº¤é‡
    df['volatility_20d'] = df.groupby('instrument')['close'].rolling(20).std().reset_index(0, drop=True)
    df['volume_ma5'] = df.groupby('instrument')['volume'].rolling(5).mean().reset_index(0, drop=True)
    df['volume_ma20'] = df.groupby('instrument')['volume'].rolling(20).mean().reset_index(0, drop=True)
    df['volume_ratio'] = df['volume_ma5'] / (df['volume_ma20'] + 1e-6)

    # RSI
    delta = df.groupby('instrument')['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / (loss + 1e-6)
    df['rsi'] = 100 - (100 / (1 + rs))

    # ç®€å•è¯„åˆ†
    for col in ['return_20d', 'volume_ratio', 'rsi']:
        if col in df.columns:
            df[f'{col}_norm'] = df.groupby('date')[col].rank(pct=True)

    df['position'] = 0
    weights = {'return_20d_norm': 0.4, 'volume_ratio_norm': 0.3, 'rsi_norm': 0.3}
    for factor, weight in weights.items():
        if factor in df.columns:
            df['position'] += df[factor].fillna(0.5) * weight

    return df


# ========== ç¬¬5éƒ¨åˆ†ï¼šä¸»æ•°æ®åŠ è½½å‡½æ•° ==========

def load_data_from_tushare(
    start_date, end_date, max_stocks=50, use_cache=True,
    cache_manager=None, use_stockranker=True,
    custom_weights=None, tushare_token=None,
    use_fundamental=True, min_days_listed=180,
    use_money_flow=True, money_flow_style='balanced'  # âœ… æ–°å¢å‚æ•°
):
    """
    ä»TushareåŠ è½½æ•°æ®å¹¶è®¡ç®—å› å­ (å†…å­˜ä¼˜åŒ–ç‰ˆ v2.6)
    
    æ–°å¢å‚æ•°:
        use_money_flow: æ˜¯å¦å¯ç”¨èµ„é‡‘æµå› å­
        money_flow_style: 'conservative' | 'balanced' | 'aggressive'
    """
    
    print("\n" + "=" * 80)
    print("ğŸ“¦ æ•°æ®åŠ è½½æ¨¡å— (å†…å­˜ä¼˜åŒ–ç‰ˆ v2.6)")
    print("=" * 80)

    # ç”Ÿæˆç¼“å­˜Key
    model_suffix = "stockranker" if use_stockranker else "simple"
    if use_fundamental: model_suffix += "_fundamental"
    if use_money_flow: model_suffix += "_moneyflow"  # âœ… æ·»åŠ èµ„é‡‘æµæ ‡è¯†
    cache_key = f"factor_data_ts_v2.6_{start_date}_{end_date}_{max_stocks}_{model_suffix}_{min_days_listed}"
    price_cache_key = f"price_data_ts_v2.6_{start_date}_{end_date}_{max_stocks}_{min_days_listed}"

    # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
    if use_cache and cache_manager:
        print("\nğŸ” æ£€æŸ¥ç¼“å­˜...")
        factor_data = cache_manager.load_from_csv(cache_key)
        price_data = cache_manager.load_from_csv(price_cache_key)
        if factor_data is not None and price_data is not None:
            print("âœ“ ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return factor_data, price_data

    # 2. åˆå§‹åŒ–æ•°æ®æº
    rate_limiter = RateLimiter(max_calls=800, time_window=60)
    data_source = TushareDataSource(
        cache_manager=cache_manager if use_cache else None,
        token=tushare_token,
        rate_limiter=rate_limiter
    )

    # 3. è·å–è‚¡ç¥¨åˆ—è¡¨
    stock_list = data_source.get_stock_list(date=start_date, min_days_listed=min_days_listed)
    if not stock_list: return None, None
    stock_list = stock_list[:max_stocks]

    # è·å–è‚¡ç¥¨ä¸Šå¸‚ä¿¡æ¯
    stock_info_df = data_source.pro.stock_basic(exchange='', list_status='L', fields='ts_code,list_date')
    stock_info_dict = dict(zip(stock_info_df['ts_code'], stock_info_df['list_date']))

    # 4. è·å–ä»·æ ¼æ•°æ®
    all_price_data = []
    print(f"\nğŸ“Š è·å– {len(stock_list)} åªè‚¡ç¥¨çš„å†å²æ•°æ®...")

    start_time = time.time()
    for i, ts_code in enumerate(stock_list):
        if (i + 1) % 10 == 0: print(f"  è¿›åº¦: {i + 1}/{len(stock_list)}")

        list_date = stock_info_dict.get(ts_code)
        df = data_source.get_price_data(ts_code, start_date, end_date, list_date=list_date)
        if df is not None: all_price_data.append(df)

    if not all_price_data: return None, None
    price_df = pd.concat(all_price_data, ignore_index=True)

    # 5. è·å–å¹¶åˆå¹¶åŸºæœ¬é¢æ•°æ®
    if use_stockranker and use_fundamental:
        print(f"\nğŸ“ˆ è·å–åŸºæœ¬é¢è´¢åŠ¡æ•°æ®...")
        all_financial_data = []
        for i, ts_code in enumerate(stock_list):
            if (i + 1) % 10 == 0: print(f"  è¿›åº¦: {i + 1}/{len(stock_list)}")
            f_df = data_source.get_financial_indicators(ts_code, start_date, end_date)
            if f_df is not None: all_financial_data.append(f_df)

        if all_financial_data:
            financial_df = pd.concat(all_financial_data, ignore_index=True)
            price_df = data_source.merge_financial_data_to_daily(price_df, financial_df)
        else:
            print("âš ï¸  æœªè·å–åˆ°åŸºæœ¬é¢æ•°æ®,å°†ä¸ä½¿ç”¨åŸºæœ¬é¢å› å­")
            use_fundamental = False

    price_df['date'] = price_df['date'].astype(str)

    # 6. è®¡ç®—å› å­
    if use_stockranker:
        model = StockRankerModel(
            custom_weights=custom_weights,
            use_fundamental=use_fundamental,
            use_money_flow=use_money_flow,        # âœ… ä¼ å…¥å‚æ•°
            money_flow_style=money_flow_style     # âœ… ä¼ å…¥å‚æ•°
        )
        factor_df = model.calculate_all_factors(price_df)
        factor_df = model.calculate_position_score(factor_df)
    else:
        print("\nâš™ï¸  è®¡ç®—ç®€å•æŠ€æœ¯å› å­...")
        factor_df = calculate_simple_factors(price_df)

    factor_df = factor_df.dropna(subset=['position'])

    # 7. æ•´ç†è¾“å‡ºåˆ—
    essential_columns = ['date', 'instrument', 'position']
    price_only_columns = ['open', 'high', 'low', 'close', 'volume', 'amount']

    # è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰å› å­åˆ— (æ’é™¤å¿…é¡»åˆ—å’Œä»·æ ¼åˆ—)
    all_columns = factor_df.columns.tolist()
    factor_columns = [col for col in all_columns if col not in essential_columns + price_only_columns]

    result_factor = factor_df[essential_columns + factor_columns].copy()

    # âœ… ä¿®å¤: ä»·æ ¼æ•°æ®ä¸åŒ…å« position åˆ—
    # price_df åŒ…å« 'amount' (ç”± get_price_data ä¿®å¤æä¾›) ä½†ä¸åŒ…å« 'position'
    price_columns_to_keep = ['date', 'instrument'] + price_only_columns

    if use_fundamental:
        for col in ['roe', 'roa', 'gross_margin', 'net_margin', 'debt_ratio']:
            if col in price_df.columns: price_columns_to_keep.append(col)

    # è¿‡æ»¤æ‰ price_df ä¸­ä¸å­˜åœ¨çš„åˆ—
    price_columns_to_keep = [col for col in price_columns_to_keep if col in price_df.columns]

    result_price = price_df[price_columns_to_keep].copy()

    # 8. è·å–å¹¶åˆå¹¶è¡Œä¸šæ•°æ®
    print("\nğŸ“Š è·å–è¡Œä¸šæ•°æ®...")
    industry_data = data_source.get_industry_data(stock_list, use_cache=use_cache)
    if industry_data is not None and not industry_data.empty:
        result_factor = result_factor.merge(industry_data, on='instrument', how='left')
        result_factor['industry'] = result_factor['industry'].fillna('å…¶ä»–')
    else:
        result_factor['industry'] = 'Unknown'

    # 9. ä¿å­˜ç¼“å­˜
    if use_cache and cache_manager:
        print("\nğŸ’¾ ä¿å­˜åˆ°ç¼“å­˜...")
        cache_manager.save_to_csv(result_factor, cache_key)
        cache_manager.save_to_csv(result_price, price_cache_key)

    data_source.print_rate_limit_stats()
    print("âœ“ æ•°æ®å‡†å¤‡å®Œæˆ")
    return result_factor, result_price


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========
if __name__ == "__main__":
    print("æ•°æ®æ¨¡å—åŠ è½½å®Œæˆã€‚è¯·åœ¨ä¸»ç¨‹åºä¸­å¯¼å…¥ä½¿ç”¨ã€‚")
"""
data_module_incremental.py - ä¿®å¤ç‰ˆ
âœ… ä¿®å¤ï¼šä¿ç•™æ‰€æœ‰å› å­åˆ—ï¼Œä¸åªæ˜¯position

å…³é”®ä¿®å¤ï¼š
- ç¬¬366è¡Œï¼šä¿ç•™æ‰€æœ‰å› å­åˆ—ä¾›æœºå™¨å­¦ä¹ ä½¿ç”¨
- å¢åŠ å› å­åˆ—ç»Ÿè®¡è¾“å‡º
"""

import pandas as pd
import numpy as np
import os
import pickle
from datetime import datetime, timedelta
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


# ============ APIé™æµå™¨ ============

class ThreadSafeRateLimiter:
    """çº¿ç¨‹å®‰å…¨çš„APIé™æµå™¨"""

    def __init__(self, max_calls_per_minute=800):
        self.max_calls = max_calls_per_minute
        self.calls = []
        self.lock = threading.Lock()

    def acquire(self):
        """è·å–è°ƒç”¨è®¸å¯"""
        with self.lock:
            now = time.time()
            self.calls = [t for t in self.calls if now - t < 60]

            if len(self.calls) >= self.max_calls:
                sleep_time = 60 - (now - self.calls[0]) + 0.1
                time.sleep(sleep_time)
                self.calls = []

            self.calls.append(time.time())


# ============ æ™ºèƒ½è‚¡ç¥¨æŠ½æ ·å™¨ ============

class SmartStockSampler:
    """æ™ºèƒ½è‚¡ç¥¨æŠ½æ ·å™¨ - æŒ‰å¸‚å€¼åˆ†å±‚æŠ½æ ·"""

    def __init__(self, data_source):
        self.data_source = data_source

    def get_stratified_sample(self, stock_list, sample_size=800):
        print(f"\n  ğŸ¯ æ™ºèƒ½æŠ½æ ·: ä» {len(stock_list)} åªä¸­é€‰æ‹© {sample_size} åª...")

        if len(stock_list) <= sample_size:
            print(f"  â„¹ï¸  è‚¡ç¥¨æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨ {len(stock_list)} åª")
            return stock_list

        try:
            pro = self.data_source.pro
            stock_info = pro.stock_basic(
                exchange='',
                list_status='L',
                fields='ts_code,name,total_mv'
            )

            stock_info = stock_info[stock_info['ts_code'].isin(stock_list)]
            stock_info = stock_info.dropna(subset=['total_mv'])

            if len(stock_info) == 0:
                print(f"  âš ï¸  æ— æ³•è·å–å¸‚å€¼ä¿¡æ¯ï¼Œä½¿ç”¨éšæœºæŠ½æ ·")
                import random
                return random.sample(stock_list, sample_size)

            stock_info = stock_info.sort_values('total_mv', ascending=False)
            total_count = len(stock_info)

            large_cap = stock_info.head(int(total_count * 0.2))
            mid_cap = stock_info.iloc[int(total_count * 0.2):int(total_count * 0.8)]
            small_cap = stock_info.tail(int(total_count * 0.2))

            n_large = int(sample_size * 0.4)
            n_mid = int(sample_size * 0.4)
            n_small = sample_size - n_large - n_mid

            sampled = pd.concat([
                large_cap.sample(n=min(n_large, len(large_cap)), random_state=42),
                mid_cap.sample(n=min(n_mid, len(mid_cap)), random_state=42),
                small_cap.sample(n=min(n_small, len(small_cap)), random_state=42)
            ])

            selected = sampled['ts_code'].tolist()

            print(f"  âœ“ æŠ½æ ·å®Œæˆ: å¤§ç›˜ {n_large}åª | ä¸­ç›˜ {n_mid}åª | å°ç›˜ {n_small}åª")
            return selected

        except Exception as e:
            print(f"  âš ï¸  æ™ºèƒ½æŠ½æ ·å¤±è´¥: {e}")
            print(f"  ä½¿ç”¨éšæœºæŠ½æ ·...")
            import random
            return random.sample(stock_list, min(sample_size, len(stock_list)))


# ============ å¹¶è¡Œæ•°æ®è·å–å™¨ ============

class ParallelDataFetcher:
    """å¤šçº¿ç¨‹å¹¶è¡Œæ•°æ®è·å–å™¨"""

    def __init__(self, data_source, max_workers=10, rate_limiter=None):
        self.data_source = data_source
        self.max_workers = max_workers
        self.rate_limiter = rate_limiter or ThreadSafeRateLimiter(max_calls_per_minute=800)

    def fetch_price_data_parallel(self, stock_list, start_date, end_date):
        print(f"\n  ğŸš€ å¤šçº¿ç¨‹è·å–ä»·æ ¼æ•°æ® ({self.max_workers}çº¿ç¨‹)...")

        all_data = []
        success_count = 0
        fail_count = 0

        def fetch_one(ts_code):
            try:
                self.rate_limiter.acquire()
                df = self.data_source.get_price_data(ts_code, start_date, end_date)
                if df is not None and len(df) > 0:
                    return ('success', df)
                return ('fail', None)
            except Exception as e:
                return ('fail', None)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(fetch_one, stock): stock for stock in stock_list}

            for i, future in enumerate(as_completed(futures)):
                status, data = future.result()

                if status == 'success':
                    all_data.append(data)
                    success_count += 1
                else:
                    fail_count += 1

                if (i + 1) % 100 == 0 or i == len(stock_list) - 1:
                    progress = (i + 1) / len(stock_list) * 100
                    print(f"    è¿›åº¦: {i + 1}/{len(stock_list)} ({progress:.1f}%) | "
                          f"æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count}")

        print(f"  âœ“ æˆåŠŸè·å– {success_count}/{len(stock_list)} åªè‚¡ç¥¨")

        if len(all_data) == 0:
            return None

        return pd.concat(all_data, ignore_index=True)

    def fetch_financial_data_parallel(self, stock_list, start_date, end_date):
        print(f"\n  ğŸš€ å¤šçº¿ç¨‹è·å–åŸºæœ¬é¢æ•°æ® ({self.max_workers}çº¿ç¨‹)...")

        all_data = []
        success_count = 0

        def fetch_one(ts_code):
            try:
                self.rate_limiter.acquire()
                df = self.data_source.get_financial_indicators(ts_code, start_date, end_date)
                if df is not None and len(df) > 0:
                    return ('success', df)
                return ('fail', None)
            except Exception as e:
                return ('fail', None)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(fetch_one, stock): stock for stock in stock_list}

            for i, future in enumerate(as_completed(futures)):
                status, data = future.result()

                if status == 'success':
                    all_data.append(data)
                    success_count += 1

                if (i + 1) % 100 == 0 or i == len(stock_list) - 1:
                    progress = (i + 1) / len(stock_list) * 100
                    print(f"    è¿›åº¦: {i + 1}/{len(stock_list)} ({progress:.1f}%) | æˆåŠŸ: {success_count}")

        print(f"  âœ“ æˆåŠŸè·å– {success_count}/{len(stock_list)} åªè‚¡ç¥¨")

        if len(all_data) == 0:
            return None

        return pd.concat(all_data, ignore_index=True)


# ============ å¢é‡æ•°æ®ç®¡ç†å™¨ ============

class IncrementalDataManager:
    """å¢é‡æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, cache_manager, data_source):
        self.cache = cache_manager
        self.data_source = data_source

        print("\n" + "=" * 80)
        print("âš¡ å¢é‡æ•°æ®æ›´æ–°ç³»ç»Ÿ")
        print("=" * 80)

    def get_cache_date_range(self, cache_name):
        cached_data = self.cache.load_from_csv(cache_name)
        if cached_data is None:
            return None

        if 'date' in cached_data.columns:
            dates = pd.to_datetime(cached_data['date'])
            return dates.min(), dates.max()

        return None

    def should_use_incremental_update(self, cache_name, target_end_date):
        date_range = self.get_cache_date_range(cache_name)

        if date_range is None:
            print("  ğŸ“¦ æœªå‘ç°ç¼“å­˜ï¼Œå°†æ‰§è¡Œå…¨é‡è·å–")
            return False, None

        cache_start, cache_end = date_range
        target_end = pd.to_datetime(target_end_date)

        days_diff = (target_end - cache_end).days

        if days_diff <= 0:
            print(f"  âœ“ ç¼“å­˜å·²æ˜¯æœ€æ–° (æˆªæ­¢ {cache_end.strftime('%Y-%m-%d')})")
            return False, cache_end

        elif days_diff <= 30:
            print(f"  âš¡ å¢é‡æ›´æ–°æ¨¡å¼: éœ€æ›´æ–° {days_diff} å¤©")
            print(f"     ç¼“å­˜æ—¥æœŸ: {cache_end.strftime('%Y-%m-%d')}")
            print(f"     ç›®æ ‡æ—¥æœŸ: {target_end_date}")
            return True, cache_end

        else:
            print(f"  âš ï¸  ç¼“å­˜è¿‡æ—§ ({days_diff} å¤©)ï¼Œå°†æ‰§è¡Œå…¨é‡è·å–")
            return False, None


# ============ ä¸»æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿®å¤ç‰ˆï¼‰============

def load_data_with_incremental_update(start_date, end_date, max_stocks=800,
                                     cache_manager=None, use_stockranker=True,
                                     custom_weights=None, tushare_token=None,
                                     use_fundamental=True, force_full_update=False,
                                     use_sampling=True, sample_size=800, max_workers=10):
    """
    ä½¿ç”¨å¢é‡æ›´æ–° + å¤šçº¿ç¨‹ + æ™ºèƒ½æŠ½æ ·åŠ è½½æ•°æ®

    âœ… ä¿®å¤ï¼šä¿ç•™æ‰€æœ‰å› å­åˆ—ï¼Œä¸åªæ˜¯position
    """
    print("\n" + "=" * 80)
    print("ğŸ“¦ æ•°æ®åŠ è½½æ¨¡å— (å¢é‡æ›´æ–° + å¤šçº¿ç¨‹ + æ™ºèƒ½æŠ½æ ·)")
    print("=" * 80)

    from data_module import TushareDataSource, StockRankerModel

    data_source = TushareDataSource(
        cache_manager=cache_manager,
        token=tushare_token
    )

    model_suffix = "stockranker" if use_stockranker else "simple"
    if use_fundamental:
        model_suffix += "_fundamental"
    if use_sampling:
        model_suffix += f"_sample{sample_size}"

    price_cache_key = f"price_data_fast_{start_date}_{end_date}_{sample_size if use_sampling else max_stocks}"
    factor_cache_key = f"factor_data_fast_{start_date}_{end_date}_{sample_size if use_sampling else max_stocks}_{model_suffix}"
    financial_cache_key = f"financial_data_fast_{start_date}_{end_date}_{sample_size if use_sampling else max_stocks}"

    incremental_mgr = IncrementalDataManager(cache_manager, data_source)

    use_incremental = False
    cache_end_date = None

    if not force_full_update and cache_manager:
        use_incremental, cache_end_date = incremental_mgr.should_use_incremental_update(
            price_cache_key, end_date
        )

    if force_full_update:
        print("  ğŸ”„ å¼ºåˆ¶å…¨é‡æ›´æ–°æ¨¡å¼")

    print("\n  ğŸ“‹ è·å–è‚¡ç¥¨åˆ—è¡¨...")
    full_stock_list = data_source.get_stock_list()
    if not full_stock_list:
        print("âœ— æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨!")
        return None, None

    print(f"  âœ“ è·å–åˆ° {len(full_stock_list)} åªè‚¡ç¥¨")

    if use_sampling and len(full_stock_list) > sample_size:
        sampler = SmartStockSampler(data_source)
        stock_list = sampler.get_stratified_sample(full_stock_list, sample_size)
    else:
        stock_list = full_stock_list[:max_stocks]
        if not use_sampling:
            print(f"  â„¹ï¸  ä¸ä½¿ç”¨æŠ½æ ·ï¼Œä½¿ç”¨å‰ {len(stock_list)} åªè‚¡ç¥¨")

    rate_limiter = ThreadSafeRateLimiter(max_calls_per_minute=800)
    fetcher = ParallelDataFetcher(data_source, max_workers=max_workers, rate_limiter=rate_limiter)

    if use_incremental and cache_end_date:
        print("\n" + "=" * 80)
        print("âš¡ å¢é‡æ›´æ–°æ¨¡å¼")
        print("=" * 80)

        print("\n  ğŸ“‚ åŠ è½½å†å²æ•°æ®...")
        old_price_data = cache_manager.load_from_csv(price_cache_key)
        old_financial_data = cache_manager.load_from_csv(financial_cache_key)

        incremental_start = (cache_end_date + timedelta(days=1)).strftime('%Y-%m-%d')
        incremental_end = end_date

        new_price_data = fetcher.fetch_price_data_parallel(
            stock_list, incremental_start, incremental_end
        )

        if new_price_data is not None and len(new_price_data) > 0:
            old_price_data['date'] = old_price_data['date'].astype(str)
            new_price_data['date'] = new_price_data['date'].astype(str)

            existing_dates = set(old_price_data['date'].unique())
            new_price_data_unique = new_price_data[~new_price_data['date'].isin(existing_dates)]

            price_df = pd.concat([old_price_data, new_price_data_unique], ignore_index=True)
            price_df = price_df.sort_values(['instrument', 'date']).reset_index(drop=True)

            print(f"  âœ“ æ•°æ®åˆå¹¶å®Œæˆ:")
            print(f"     å†å²æ•°æ®: {len(old_price_data)} æ¡")
            print(f"     æ–°å¢æ•°æ®: {len(new_price_data_unique)} æ¡")
            print(f"     åˆå¹¶æ€»è®¡: {len(price_df)} æ¡")
        else:
            print("  âš ï¸  æœªè·å–åˆ°æ–°æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®")
            price_df = old_price_data

        financial_df = old_financial_data
        if use_fundamental:
            cache_quarter = pd.Period(cache_end_date, freq='Q')
            target_quarter = pd.Period(end_date, freq='Q')

            if target_quarter > cache_quarter:
                print(f"\n  ğŸ“ˆ è·¨å­£åº¦ï¼Œæ›´æ–°åŸºæœ¬é¢æ•°æ®...")
                new_financial = fetcher.fetch_financial_data_parallel(
                    stock_list, incremental_start, incremental_end
                )

                if new_financial is not None and len(new_financial) > 0:
                    if old_financial_data is not None:
                        financial_df = pd.concat([old_financial_data, new_financial], ignore_index=True)
                        financial_df = financial_df.drop_duplicates(subset=['instrument', 'date'], keep='last')
                    else:
                        financial_df = new_financial
                    print(f"  âœ“ åŸºæœ¬é¢æ•°æ®å·²æ›´æ–°")
            else:
                print(f"  â„¹ï¸  æœªè·¨å­£åº¦ï¼ŒåŸºæœ¬é¢æ•°æ®æ— éœ€æ›´æ–°")

    else:
        print("\n" + "=" * 80)
        print("ğŸ“¥ å…¨é‡è·å–æ¨¡å¼")
        print("=" * 80)

        price_df = fetcher.fetch_price_data_parallel(stock_list, start_date, end_date)

        if price_df is None or len(price_df) == 0:
            print("âœ— æœªè·å–åˆ°ä»»ä½•æ•°æ®!")
            return None, None

        financial_df = None
        if use_fundamental:
            financial_df = fetcher.fetch_financial_data_parallel(stock_list, start_date, end_date)

    if use_fundamental and financial_df is not None:
        financial_df = financial_df.dropna(subset=['date', 'instrument'])

        if len(financial_df) > 0:
            print("\n  ğŸ”— åˆå¹¶åŸºæœ¬é¢æ•°æ®åˆ°æ—¥çº¿...")
            price_df = data_source.merge_financial_data_to_daily(price_df, financial_df)
            print("  âœ“ åŸºæœ¬é¢æ•°æ®åˆå¹¶å®Œæˆ")

            fundamental_cols = ['roe', 'roa', 'gross_margin', 'net_margin', 'debt_ratio']
            available_cols = [col for col in fundamental_cols if col in price_df.columns]
            if available_cols:
                coverage = (price_df[available_cols].notna().any(axis=1).sum() / len(price_df)) * 100
                print(f"     è¦†ç›–ç‡: {coverage:.1f}%")

    price_df['date'] = price_df['date'].astype(str)

    if use_stockranker:
        model = StockRankerModel(
            custom_weights=custom_weights,
            use_fundamental=use_fundamental
        )
        factor_df = model.calculate_all_factors(price_df)
        factor_df = model.calculate_position_score(factor_df)
    else:
        from data_module import calculate_simple_factors
        factor_df = calculate_simple_factors(price_df)

    factor_df = factor_df.dropna(subset=['position'])

    # âœ… å…³é”®ä¿®å¤ï¼šä¿ç•™æ‰€æœ‰å› å­åˆ—ï¼Œä¸åªæ˜¯position
    # æ’é™¤ä»·æ ¼åˆ—å’Œä¸€äº›å†—ä½™åˆ—
    exclude_cols = ['open', 'high', 'low', 'close', 'volume', 'amount', 'pre_close', 
                    'change', 'pct_chg', 'turnover_rate']
    
    # ä¿ç•™æ‰€æœ‰éæ’é™¤çš„åˆ—
    keep_cols = [col for col in factor_df.columns if col not in exclude_cols]
    result_factor = factor_df[keep_cols].copy()
    
    result_price = price_df.copy()

    if cache_manager:
        print("\n  ğŸ’¾ ä¿å­˜åˆ°ç¼“å­˜...")
        cache_manager.save_to_csv(result_price, price_cache_key)
        cache_manager.save_to_csv(result_factor, factor_cache_key)
        if financial_df is not None:
            cache_manager.save_to_csv(financial_df, financial_cache_key)

    # âœ… ç»Ÿè®¡å› å­åˆ—ä¿¡æ¯
    factor_columns = [col for col in result_factor.columns 
                      if col not in ['date', 'instrument', 'position']]
    
    print(f"\nâœ“ æ•°æ®å‡†å¤‡å®Œæˆ:")
    print(f"  - å› å­æ•°æ®: {len(result_factor)} æ¡")
    print(f"  - ä»·æ ¼æ•°æ®: {len(result_price)} æ¡")
    print(f"  - è‚¡ç¥¨æ•°é‡: {result_factor['instrument'].nunique()} åª")
    print(f"  - äº¤æ˜“æ—¥æ•°: {result_factor['date'].nunique()} å¤©")
    print(f"  - å› å­åˆ—æ•°: {len(factor_columns)} ä¸ª")
    
    if len(factor_columns) > 0:
        print(f"  - å› å­åˆ—è¡¨: {', '.join(factor_columns[:10])}{'...' if len(factor_columns) > 10 else ''}")

    if use_incremental and cache_end_date:
        days_added = (pd.to_datetime(end_date) - cache_end_date).days
        print(f"  - æ–°å¢å¤©æ•°: {days_added} å¤© âš¡")

    if use_sampling:
        print(f"  - æŠ½æ ·æ–¹å¼: å¸‚å€¼åˆ†å±‚æŠ½æ ·")

    return result_factor, result_price